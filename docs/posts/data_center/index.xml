<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DataCenter on Website of OString</title>
    <link>http://localhost:1313/posts/data_center/</link>
    <description>Recent content in DataCenter on Website of OString</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 22 May 2025 22:16:08 +0800</lastBuildDate><atom:link href="http://localhost:1313/posts/data_center/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>http://localhost:1313/posts/data_center/nvidia_fusion/nvdia_fusion/</link>
      <pubDate>Fri, 23 May 2025 11:46:26 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/data_center/nvidia_fusion/nvdia_fusion/</guid>
      
        <description>&lt;h1 id=&#34;nvidia-fusion&#34;&gt;Nvidia Fusion&lt;/h1&gt;
&lt;p&gt;Jenson Huang 今天抛出了绣球，接下来第一个问题：其他厂商家的黄花闺女能否看的上。
另一个问题：如果有一天真的成家了，亲家之间是否会闹的鸡飞狗跳？
这颗绣球就是今天刚刚在computex 上发布的 Nvidia Fusion。英伟达发布了一系列围绕数据中心和企业级 AI 计划的公告，其中包括新推出的 NVLink Fusion 计划。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;nvlink_fusion.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;先回答第一个问题&#34;&gt;先回答第一个问题&lt;/h2&gt;
&lt;p&gt;从公开的消息看，目前接下这颗绣球的有几家厂商：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;英伟达已为该计划聚集了众多合作伙伴，包括高通和富士通，它们将把这项技术集成到各自的 CPU 中。NVLink Fusion 还将扩展至定制 AI 加速器领域，因此英伟达已吸引包括 Marvell、联发科在内的多家芯片合作伙伴，以及芯片软件设计公司新思科技（Synopsys）和楷登电子（Cadence）加入 NVLink Fusion 生态系统。&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;高通最近证实，其正在将自有定制服务器 CPU 推向市场，尽管细节仍不明确，但该公司与 NVLink 生态系统的合作将使其新 CPU 能够搭乘英伟达快速扩张的 AI 生态系统的顺风车。&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;富士通也一直在致力于将其搭载 3D 堆叠 CPU 核心和内存的 144 核 Monaka CPU 推向市场。富士通 CTO Vivek Mahajan 表示：“富士通的下一代处理器 FUJITSU-MONAKA 是一款基于 Arm 架构的 2 纳米 CPU，旨在实现极高的能效。将我们的技术与英伟达架构直接连接，标志着我们在通过世界领先的计算技术推动 AI 发展的愿景中迈出了重要一步，为新型可扩展、自主且可持续的 AI 系统铺平了道路。”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;从目前来看，Nvlink 可撬动的生态还只局限在Die2Die 的范围，在Die2Die 互联的选择上，Nvlink 不是唯一选择，另外也不是最优解。联发科已经和NVdia 有过深入合作，之前的&lt;code&gt;GB10&lt;/code&gt;现在看来已经试水了Nvlink fusion生态合作模式，由联发科基于Arm SoC CPU 的功底，结合Blackwell 的GPU 通过Nvlink 互联。另外一家Marvell，基本是全开放生态，各种联盟都来者不拒，本就是CSP chip Maker，工具箱越丰富越好。说白了，通过联发科的合作先打个窝，吸引其他厂商，在D2D 范围内还处于刚起步的状态。&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/posts/data_center/server_cooling_evolution/server_cooling_evolution/</link>
      <pubDate>Thu, 22 May 2025 22:12:04 +0800</pubDate>
      
      <guid>http://localhost:1313/posts/data_center/server_cooling_evolution/server_cooling_evolution/</guid>
      
        <description>&lt;h1 id=&#34;服务器散热进化史&#34;&gt;服务器散热进化史&lt;/h1&gt;
&lt;h3 id=&#34;液冷方案崛起&#34;&gt;液冷方案崛起&lt;/h3&gt;
&lt;p&gt;最近在关注美股，围观行业大佬分析NV 供应链体系，注意到一家公司VRT， 这家公司是NV 的独家液冷方案提供商。最近VRT 的股价青云直上，一家做液冷方案的公司护城河到底有多深，散热方案在整个行业中处于什么位置，这些问题是我想了解的。&lt;/p&gt;
&lt;p&gt;NV 的技术路线图从A100/H100 到最新GB200 的体系，TDP 也从300~700W 跃升到1000W 的门槛，以往的风冷方案在散热效率上显得非常力不从心。从2019 年的数据来看，国内IDC 能耗43%用于 IT 设备散热， 基本与 45%的 IT 设备自身能耗持平，这在低碳的大背景下，与IDC 行业追求的PUE 小于1.3 相背离。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;全国数据中心 PUE 平均水平为 1.49，传统风冷数据中心 PUE 在 1.5&lt;del&gt;1.8（1kW 的服务器正常运转需 1.5&lt;/del&gt;1.8kW 电量）&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;传统风冷&#34;&gt;传统风冷&lt;/h3&gt;
&lt;p&gt;从目前所有可见的消息来看，提高目前IDC风冷方案散热效率都是大势所趋，液冷方案呼之欲出，想要部署密度极高机架（30kW以上）的设施在是否使用液冷方面几乎没有选择余地。无论如何配置或优化系统，风冷都无法提供维持IT系统可靠性所需的散热能力。在边缘计算和核心数据中心都是如此。我们先来看看液冷方案相比于风冷有哪些优势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;液体导热能力是空气的 25 倍，同体积液体带走热量是同体积空气的近 3000 倍，液冷技术可实现40~55℃高温供液，无需压缩机冷水机组，采用室外冷却塔，可实现全年自然冷却&lt;/li&gt;
&lt;li&gt;在耗电量方面，液冷系统约比风冷系统节省电量 30~50%&lt;/li&gt;
&lt;li&gt;生态环境的优势：液冷噪音低，能够降低冷却风机转速或者采用无风机设计，从而具备极佳的降噪效果，提升机房运维环境舒适性，解决噪声污染问题&lt;/li&gt;
&lt;li&gt;液冷系统可以更高效地回收和再利用余热，有助于提升能源的总体利用效率&lt;/li&gt;
&lt;li&gt;液冷服务器可以排除海拔、地域和气 温的差距，保证运行效率和性能，具备规模化应用的优势。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://i-blog.csdnimg.cn/blog_migrate/c6290c440c9c31486781c45011396283.png&#34; alt=&#34;在这里插入图片描述&#34;&gt;
相比于风冷有这么多优势，为什么没有在IDC 发展初期就大范围铺开，从IDC 的发展历史来看，初期，一台1U 的刀片服务器主要高密度计算还集中在逻辑控制任务，再到后来的互联网时代，思科的崛起，IDC 服务器主要处理的是路由转发任务，这两类任务的一个显著特点是还集中在逻辑判断和少量的计算，映射到芯片上的逻辑单元是ALU 和开关电路，从芯片的工作状态来说，这类计算相关的能量负载是非集中式的，同时能量开销也很低。因此负担这类计算所需要的散热需求也不是难么急切，大多状况下，IDC 中的服务器散热可能主要依赖几台挂式空调就好，部署难度极地，从IT 运维的收益来说，并没有动力去推动部署液冷。
&lt;img src=&#34;https://i-blog.csdnimg.cn/blog_migrate/3ddd8bab804ff39e885f8b344dc0fa70.png&#34; alt=&#34;在这里插入图片描述&#34;&gt;
从图中可以看到，芯片计算单元（MAC/ALU）小号的能量相比于DRAM数据搬运来说，小号的能量差了4个数量级，当下的智算中心所部署的服务器特点集中体现在高带宽（HBM/GDDR）,通常数据搬运的开销是整个网络执行的瓶颈所在。&lt;/p&gt;
&lt;h3 id=&#34;液冷的护城河&#34;&gt;液冷的护城河&lt;/h3&gt;
&lt;p&gt;回到液冷的讨论，当下的液冷解决方案供应商的技术护城河体现在哪里，为什么vrt 可以给到这么高的估值，并成为NV的唯一指定供应商。&lt;/p&gt;
&lt;p&gt;目前主流的几种液冷方案：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;液冷技术类型&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;特点&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;冷板式&lt;/td&gt;
          &lt;td&gt;将液冷冷板固定在服务器的主要发热器件上，依靠流经冷板的液体将热量带走达到散热目的。&lt;/td&gt;
          &lt;td&gt;目前主流，占比超80%&lt;img src=&#34;https://i-blog.csdnimg.cn/blog_migrate/eed5dc82a5d2a72611d9f51f68a31043.png&#34; alt=&#34;在这里插入图片描述&#34;&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;浸没式&lt;/td&gt;
          &lt;td&gt;将发热元件直接浸没在冷却液中，通过冷却液循环带走服务器等设备运行产生的热量。分类：单相、两相、直抵芯片&lt;/td&gt;
          &lt;td&gt;未来大趋势&lt;img src=&#34;https://i-blog.csdnimg.cn/blog_migrate/541140c7db0226a0a203a7b83c9bc58b.png&#34; alt=&#34;在这里插入图片描述&#34;&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;喷淋式&lt;/td&gt;
          &lt;td&gt;在机箱顶部储液和开孔，根据发热体位置和发热量大小不同，让冷却液对发热体进行喷淋，达到设备冷却的目的。&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;关键技术&lt;/th&gt;
          &lt;th&gt;作用&lt;/th&gt;
          &lt;th&gt;详情&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;冷却液&lt;/td&gt;
          &lt;td&gt;可用冷媒包括水、矿物油、电子氟化液等进行物理降温&lt;/td&gt;
          &lt;td&gt;冷却液是关键的原材料之一。以前，冷却液基本上被3M为代表的国外企业垄断，如今国内企业在冷却液方面也取得了国产突破。巨化集团推出高性能巨芯专用冷却液,攻克了数据中心高效液冷与安全智能热控防护等关键技术，并实现批量生产。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;连接器&lt;/td&gt;
          &lt;td&gt;通常是指液冷循环系统中各器件之间的连接件，它能 实现各器件之间的快速连接和断开且无泄漏，提高效率，减少排液注液带来的不必要的工作量。主 要应用于流体连接，可在带压状态下自由插拔且无泄漏。&lt;/td&gt;
          &lt;td&gt;液冷快速连接器的可靠性和绝对密封性至关重要，其事关整个液冷系统的安全性，使得其具备较高的技术门槛和较多的技术要点，包括防水密封、高耐用度及材质兼容性等&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;CDU（Cooling Dispensing Unit，冷却液分配单元&lt;/td&gt;
          &lt;td&gt;主要由机箱、水泵、板换、阀、膨胀罐以及 管路等等组成，通过板换进行热量交换，把冷却的液体送到热源处吸收热量，带着热量的液体再进 入板换进行换热处理，循环换热。&lt;/td&gt;
          &lt;td&gt;&lt;img src=&#34;https://i-blog.csdnimg.cn/blog_migrate/1c3d1ea38ff2d61bf00f564a7bbe7542.png&#34; alt=&#34;在这里插入图片描述&#34;&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;电磁阀&lt;/td&gt;
          &lt;td&gt;一般安装在连接的管道上控制冷媒流通。&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;TANK&lt;/td&gt;
          &lt;td&gt;用于安装服务器/交换机的浸没式箱体，通过 tank 内的冷却介质直接对交换机进行散热&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Manifold&lt;/td&gt;
          &lt;td&gt;分集水器，用于连接各路加热管供回水的配、集水装臵，按进、回水方式不同分为分 水器和集水器。&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;国内已经形成了较为完整的液冷数据中心产业链，不过由于上游零部件的工艺性能、技术水平等方面与进口产品还是存在差距，液冷数据中心产业链有待完善。液冷技术非常复杂，涉及液冷数据中心系统架构层、液冷部件及接口层、液冷基础设施层（液冷机柜、组件、换热设备、室外集成冷源等）、液冷监控系统层等多方面，产业链上各个企业技术路径多种多样、产品规格千差万别，产品质量良莠不齐，让用户难以选择。&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>