<!doctype html>
<html
  dir="ltr"
  lang="en"
  data-theme=""
  
    class="html theme--light"
  
><head>
  <meta charset="utf-8" />
  <title>
    OString

    

  </title>

  <meta name="generator" content="Hugo 0.147.4"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <meta name="author" content="OString" />
  <meta
    name="description"
    content="Welcome to OString&#39;s Website"
  />
  
  
    
    
    <link
      rel="stylesheet"
      href="/scss/main.min.8d4fad7e6916ad2e291e8d97ada157c70350d6d7150fea137e7243340967befb.css"
      integrity="sha256-jU&#43;tfmkWrS4pHo2XraFXxwNQ1tcVD&#43;oTfnJDNAlnvvs="
      crossorigin="anonymous"
      type="text/css"
    />
  

  
  <link
    rel="stylesheet"
    href="/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css"
    integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA="
    crossorigin="anonymous"
    type="text/css"
  />
  
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/fontawesome.min.137b1cf3cea9a8adb7884343a9a5ddddf4280f59153f74dc782fb7f7bf0d0519.css"
    integrity="sha256-E3sc886pqK23iENDqaXd3fQoD1kVP3TceC&#43;3978NBRk="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/solid.min.e65dc5b48fb5f39b142360c57c3a215744c94e56c755c929cc3e88fe12aab4d3.css"
    integrity="sha256-5l3FtI&#43;185sUI2DFfDohV0TJTlbHVckpzD6I/hKqtNM="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/regular.min.6f4f16d58da1c82c0c3a3436e021a3d39b4742f741192c546e73e947eacfd92f.css"
    integrity="sha256-b08W1Y2hyCwMOjQ24CGj05tHQvdBGSxUbnPpR&#43;rP2S8="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/brands.min.e10425ad768bc98ff1fb272a0ac8420f9d1ba22f0612c08ff1010c95080ffe7e.css"
    integrity="sha256-4QQlrXaLyY/x&#43;ycqCshCD50boi8GEsCP8QEMlQgP/n4="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />

  <link rel="canonical" href="https://ostring.github.io/posts/data_center/server_cooling_evolution/server_cooling_evolution/" />
  
  
  
  
  <script
    type="text/javascript"
    src="/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js"
    integrity="sha256-&#43;RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI="
    crossorigin="anonymous"
  ></script>

  
    
    
    <script
      type="text/javascript"
      src="/js/anatole-theme-switcher.min.8724ddf9268dee451060f191961647573c7f592fbccc6d858746236b3f915813.js"
      integrity="sha256-hyTd&#43;SaN7kUQYPGRlhZHVzx/WS&#43;8zG2Fh0Yjaz&#43;RWBM="
      crossorigin="anonymous"
    ></script>
  

  

  

  


  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Website of OString">
  <meta name="twitter:description" content="服务器散热进化史 液冷方案崛起 最近在关注美股，围观行业大佬分析NV 供应链体系，注意到一家公司VRT， 这家公司是NV 的独家液冷方案提供商。最近VRT 的股价青云直上，一家做液冷方案的公司护城河到底有多深，散热方案在整个行业中处于什么位置，这些问题是我想了解的。
NV 的技术路线图从A100/H100 到最新GB200 的体系，TDP 也从300~700W 跃升到1000W 的门槛，以往的风冷方案在散热效率上显得非常力不从心。从2019 年的数据来看，国内IDC 能耗43%用于 IT 设备散热， 基本与 45%的 IT 设备自身能耗持平，这在低碳的大背景下，与IDC 行业追求的PUE 小于1.3 相背离。
全国数据中心 PUE 平均水平为 1.49，传统风冷数据中心 PUE 在 1.51.8（1kW 的服务器正常运转需 1.51.8kW 电量）
传统风冷 从目前所有可见的消息来看，提高目前IDC风冷方案散热效率都是大势所趋，液冷方案呼之欲出，想要部署密度极高机架（30kW以上）的设施在是否使用液冷方面几乎没有选择余地。无论如何配置或优化系统，风冷都无法提供维持IT系统可靠性所需的散热能力。在边缘计算和核心数据中心都是如此。我们先来看看液冷方案相比于风冷有哪些优势：
液体导热能力是空气的 25 倍，同体积液体带走热量是同体积空气的近 3000 倍，液冷技术可实现40~55℃高温供液，无需压缩机冷水机组，采用室外冷却塔，可实现全年自然冷却 在耗电量方面，液冷系统约比风冷系统节省电量 30~50% 生态环境的优势：液冷噪音低，能够降低冷却风机转速或者采用无风机设计，从而具备极佳的降噪效果，提升机房运维环境舒适性，解决噪声污染问题 液冷系统可以更高效地回收和再利用余热，有助于提升能源的总体利用效率 液冷服务器可以排除海拔、地域和气 温的差距，保证运行效率和性能，具备规模化应用的优势。 相比于风冷有这么多优势，为什么没有在IDC 发展初期就大范围铺开，从IDC 的发展历史来看，初期，一台1U 的刀片服务器主要高密度计算还集中在逻辑控制任务，再到后来的互联网时代，思科的崛起，IDC 服务器主要处理的是路由转发任务，这两类任务的一个显著特点是还集中在逻辑判断和少量的计算，映射到芯片上的逻辑单元是ALU 和开关电路，从芯片的工作状态来说，这类计算相关的能量负载是非集中式的，同时能量开销也很低。因此负担这类计算所需要的散热需求也不是难么急切，大多状况下，IDC 中的服务器散热可能主要依赖几台挂式空调就好，部署难度极地，从IT 运维的收益来说，并没有动力去推动部署液冷。 从图中可以看到，芯片计算单元（MAC/ALU）小号的能量相比于DRAM数据搬运来说，小号的能量差了4个数量级，当下的智算中心所部署的服务器特点集中体现在高带宽（HBM/GDDR）,通常数据搬运的开销是整个网络执行的瓶颈所在。
液冷的护城河 回到液冷的讨论，当下的液冷解决方案供应商的技术护城河体现在哪里，为什么vrt 可以给到这么高的估值，并成为NV的唯一指定供应商。
目前主流的几种液冷方案：
液冷技术类型 描述 特点 冷板式 将液冷冷板固定在服务器的主要发热器件上，依靠流经冷板的液体将热量带走达到散热目的。 目前主流，占比超80% 浸没式 将发热元件直接浸没在冷却液中，通过冷却液循环带走服务器等设备运行产生的热量。分类：单相、两相、直抵芯片 未来大趋势 喷淋式 在机箱顶部储液和开孔，根据发热体位置和发热量大小不同，让冷却液对发热体进行喷淋，达到设备冷却的目的。 关键技术 作用 详情 冷却液 可用冷媒包括水、矿物油、电子氟化液等进行物理降温 冷却液是关键的原材料之一。以前，冷却液基本上被3M为代表的国外企业垄断，如今国内企业在冷却液方面也取得了国产突破。巨化集团推出高性能巨芯专用冷却液,攻克了数据中心高效液冷与安全智能热控防护等关键技术，并实现批量生产。 连接器 通常是指液冷循环系统中各器件之间的连接件，它能 实现各器件之间的快速连接和断开且无泄漏，提高效率，减少排液注液带来的不必要的工作量。主 要应用于流体连接，可在带压状态下自由插拔且无泄漏。 液冷快速连接器的可靠性和绝对密封性至关重要，其事关整个液冷系统的安全性，使得其具备较高的技术门槛和较多的技术要点，包括防水密封、高耐用度及材质兼容性等 CDU（Cooling Dispensing Unit，冷却液分配单元 主要由机箱、水泵、板换、阀、膨胀罐以及 管路等等组成，通过板换进行热量交换，把冷却的液体送到热源处吸收热量，带着热量的液体再进 入板换进行换热处理，循环换热。 电磁阀 一般安装在连接的管道上控制冷媒流通。 TANK 用于安装服务器/交换机的浸没式箱体，通过 tank 内的冷却介质直接对交换机进行散热 Manifold 分集水器，用于连接各路加热管供回水的配、集水装臵，按进、回水方式不同分为分 水器和集水器。 国内已经形成了较为完整的液冷数据中心产业链，不过由于上游零部件的工艺性能、技术水平等方面与进口产品还是存在差距，液冷数据中心产业链有待完善。液冷技术非常复杂，涉及液冷数据中心系统架构层、液冷部件及接口层、液冷基础设施层（液冷机柜、组件、换热设备、室外集成冷源等）、液冷监控系统层等多方面，产业链上各个企业技术路径多种多样、产品规格千差万别，产品质量良莠不齐，让用户难以选择。">



  
  <meta property="og:url" content="https://ostring.github.io/posts/data_center/server_cooling_evolution/server_cooling_evolution/">
  <meta property="og:site_name" content="Website of OString">
  <meta property="og:title" content="Website of OString">
  <meta property="og:description" content="服务器散热进化史 液冷方案崛起 最近在关注美股，围观行业大佬分析NV 供应链体系，注意到一家公司VRT， 这家公司是NV 的独家液冷方案提供商。最近VRT 的股价青云直上，一家做液冷方案的公司护城河到底有多深，散热方案在整个行业中处于什么位置，这些问题是我想了解的。
NV 的技术路线图从A100/H100 到最新GB200 的体系，TDP 也从300~700W 跃升到1000W 的门槛，以往的风冷方案在散热效率上显得非常力不从心。从2019 年的数据来看，国内IDC 能耗43%用于 IT 设备散热， 基本与 45%的 IT 设备自身能耗持平，这在低碳的大背景下，与IDC 行业追求的PUE 小于1.3 相背离。
全国数据中心 PUE 平均水平为 1.49，传统风冷数据中心 PUE 在 1.51.8（1kW 的服务器正常运转需 1.51.8kW 电量）
传统风冷 从目前所有可见的消息来看，提高目前IDC风冷方案散热效率都是大势所趋，液冷方案呼之欲出，想要部署密度极高机架（30kW以上）的设施在是否使用液冷方面几乎没有选择余地。无论如何配置或优化系统，风冷都无法提供维持IT系统可靠性所需的散热能力。在边缘计算和核心数据中心都是如此。我们先来看看液冷方案相比于风冷有哪些优势：
液体导热能力是空气的 25 倍，同体积液体带走热量是同体积空气的近 3000 倍，液冷技术可实现40~55℃高温供液，无需压缩机冷水机组，采用室外冷却塔，可实现全年自然冷却 在耗电量方面，液冷系统约比风冷系统节省电量 30~50% 生态环境的优势：液冷噪音低，能够降低冷却风机转速或者采用无风机设计，从而具备极佳的降噪效果，提升机房运维环境舒适性，解决噪声污染问题 液冷系统可以更高效地回收和再利用余热，有助于提升能源的总体利用效率 液冷服务器可以排除海拔、地域和气 温的差距，保证运行效率和性能，具备规模化应用的优势。 相比于风冷有这么多优势，为什么没有在IDC 发展初期就大范围铺开，从IDC 的发展历史来看，初期，一台1U 的刀片服务器主要高密度计算还集中在逻辑控制任务，再到后来的互联网时代，思科的崛起，IDC 服务器主要处理的是路由转发任务，这两类任务的一个显著特点是还集中在逻辑判断和少量的计算，映射到芯片上的逻辑单元是ALU 和开关电路，从芯片的工作状态来说，这类计算相关的能量负载是非集中式的，同时能量开销也很低。因此负担这类计算所需要的散热需求也不是难么急切，大多状况下，IDC 中的服务器散热可能主要依赖几台挂式空调就好，部署难度极地，从IT 运维的收益来说，并没有动力去推动部署液冷。 从图中可以看到，芯片计算单元（MAC/ALU）小号的能量相比于DRAM数据搬运来说，小号的能量差了4个数量级，当下的智算中心所部署的服务器特点集中体现在高带宽（HBM/GDDR）,通常数据搬运的开销是整个网络执行的瓶颈所在。
液冷的护城河 回到液冷的讨论，当下的液冷解决方案供应商的技术护城河体现在哪里，为什么vrt 可以给到这么高的估值，并成为NV的唯一指定供应商。
目前主流的几种液冷方案：
液冷技术类型 描述 特点 冷板式 将液冷冷板固定在服务器的主要发热器件上，依靠流经冷板的液体将热量带走达到散热目的。 目前主流，占比超80% 浸没式 将发热元件直接浸没在冷却液中，通过冷却液循环带走服务器等设备运行产生的热量。分类：单相、两相、直抵芯片 未来大趋势 喷淋式 在机箱顶部储液和开孔，根据发热体位置和发热量大小不同，让冷却液对发热体进行喷淋，达到设备冷却的目的。 关键技术 作用 详情 冷却液 可用冷媒包括水、矿物油、电子氟化液等进行物理降温 冷却液是关键的原材料之一。以前，冷却液基本上被3M为代表的国外企业垄断，如今国内企业在冷却液方面也取得了国产突破。巨化集团推出高性能巨芯专用冷却液,攻克了数据中心高效液冷与安全智能热控防护等关键技术，并实现批量生产。 连接器 通常是指液冷循环系统中各器件之间的连接件，它能 实现各器件之间的快速连接和断开且无泄漏，提高效率，减少排液注液带来的不必要的工作量。主 要应用于流体连接，可在带压状态下自由插拔且无泄漏。 液冷快速连接器的可靠性和绝对密封性至关重要，其事关整个液冷系统的安全性，使得其具备较高的技术门槛和较多的技术要点，包括防水密封、高耐用度及材质兼容性等 CDU（Cooling Dispensing Unit，冷却液分配单元 主要由机箱、水泵、板换、阀、膨胀罐以及 管路等等组成，通过板换进行热量交换，把冷却的液体送到热源处吸收热量，带着热量的液体再进 入板换进行换热处理，循环换热。 电磁阀 一般安装在连接的管道上控制冷媒流通。 TANK 用于安装服务器/交换机的浸没式箱体，通过 tank 内的冷却介质直接对交换机进行散热 Manifold 分集水器，用于连接各路加热管供回水的配、集水装臵，按进、回水方式不同分为分 水器和集水器。 国内已经形成了较为完整的液冷数据中心产业链，不过由于上游零部件的工艺性能、技术水平等方面与进口产品还是存在差距，液冷数据中心产业链有待完善。液冷技术非常复杂，涉及液冷数据中心系统架构层、液冷部件及接口层、液冷基础设施层（液冷机柜、组件、换热设备、室外集成冷源等）、液冷监控系统层等多方面，产业链上各个企业技术路径多种多样、产品规格千差万别，产品质量良莠不齐，让用户难以选择。">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-22T22:12:04+08:00">
    <meta property="article:modified_time" content="2025-05-22T22:12:04+08:00">



  
  
  
  
  <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "articleSection": "posts",
        "name": "",
        "headline": "",
        "alternativeHeadline": "",
        "description": "
      
        \u003ch1 id=\u0022服务器散热进化史\u0022\u003e服务器散热进化史\u003c\/h1\u003e\n\u003ch3 id=\u0022液冷方案崛起\u0022\u003e液冷方案崛起\u003c\/h3\u003e\n\u003cp\u003e最近在关注美股，围观行业大佬分析NV 供应链体系，注意到一家公司VRT， 这家公司是NV 的独家液冷方案提供商。最近VRT 的股价青云直上，一家做液冷方案的公司护城河到底有多深，散热方案在整个行业中处于什么位置，这些问题是我想了解的。\u003c\/p\u003e\n\u003cp\u003eNV 的技术路线图从A100\/H100 到最新GB200 的体系，TDP 也从300~700W 跃升到1000W 的门槛，以往的风冷方案在散热效率上显得非常力不从心。从2019 年的数据来看，国内IDC 能耗43%用于 IT 设备散热， 基本与 45%的 IT 设备自身能耗持平，这在低碳的大背景下，与IDC 行业追求的PUE 小于1.3 相背离。\u003c\/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e全国数据中心 PUE 平均水平为 1.49，传统风冷数据中心 PUE 在 1.5\u003cdel\u003e1.8（1kW 的服务器正常运转需 1.5\u003c\/del\u003e1.8kW 电量）\u003c\/p\u003e\u003c\/blockquote\u003e\n\u003ch3 id=\u0022传统风冷\u0022\u003e传统风冷\u003c\/h3\u003e\n\u003cp\u003e从目前所有可见的消息来看，提高目前IDC风冷方案散热效率都是大势所趋，液冷方案呼之欲出，想要部署密度极高机架（30kW以上）的设施在是否使用液冷方面几乎没有选择余地。无论如何配置或优化系统，风冷都无法提供维持IT系统可靠性所需的散热能力。在边缘计算和核心数据中心都是如此。我们先来看看液冷方案相比于风冷有哪些优势：\u003c\/p\u003e\n\u003cul\u003e\n\u003cli\u003e液体导热能力是空气的 25 倍，同体积液体带走热量是同体积空气的近 3000 倍，液冷技术可实现40~55℃高温供液，无需压缩机冷水机组，采用室外冷却塔，可实现全年自然冷却\u003c\/li\u003e\n\u003cli\u003e在耗电量方面，液冷系统约比风冷系统节省电量 30~50%\u003c\/li\u003e\n\u003cli\u003e生态环境的优势：液冷噪音低，能够降低冷却风机转速或者采用无风机设计，从而具备极佳的降噪效果，提升机房运维环境舒适性，解决噪声污染问题\u003c\/li\u003e\n\u003cli\u003e液冷系统可以更高效地回收和再利用余热，有助于提升能源的总体利用效率\u003c\/li\u003e\n\u003cli\u003e液冷服务器可以排除海拔、地域和气 温的差距，保证运行效率和性能，具备规模化应用的优势。\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003cp\u003e\u003cimg src=\u0022https:\/\/i-blog.csdnimg.cn\/blog_migrate\/c6290c440c9c31486781c45011396283.png\u0022 alt=\u0022在这里插入图片描述\u0022\u003e\n相比于风冷有这么多优势，为什么没有在IDC 发展初期就大范围铺开，从IDC 的发展历史来看，初期，一台1U 的刀片服务器主要高密度计算还集中在逻辑控制任务，再到后来的互联网时代，思科的崛起，IDC 服务器主要处理的是路由转发任务，这两类任务的一个显著特点是还集中在逻辑判断和少量的计算，映射到芯片上的逻辑单元是ALU 和开关电路，从芯片的工作状态来说，这类计算相关的能量负载是非集中式的，同时能量开销也很低。因此负担这类计算所需要的散热需求也不是难么急切，大多状况下，IDC 中的服务器散热可能主要依赖几台挂式空调就好，部署难度极地，从IT 运维的收益来说，并没有动力去推动部署液冷。\n\u003cimg src=\u0022https:\/\/i-blog.csdnimg.cn\/blog_migrate\/3ddd8bab804ff39e885f8b344dc0fa70.png\u0022 alt=\u0022在这里插入图片描述\u0022\u003e\n从图中可以看到，芯片计算单元（MAC\/ALU）小号的能量相比于DRAM数据搬运来说，小号的能量差了4个数量级，当下的智算中心所部署的服务器特点集中体现在高带宽（HBM\/GDDR）,通常数据搬运的开销是整个网络执行的瓶颈所在。\u003c\/p\u003e\n\u003ch3 id=\u0022液冷的护城河\u0022\u003e液冷的护城河\u003c\/h3\u003e\n\u003cp\u003e回到液冷的讨论，当下的液冷解决方案供应商的技术护城河体现在哪里，为什么vrt 可以给到这么高的估值，并成为NV的唯一指定供应商。\u003c\/p\u003e\n\u003cp\u003e目前主流的几种液冷方案：\u003c\/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e液冷技术类型\u003c\/th\u003e\n          \u003cth\u003e描述\u003c\/th\u003e\n          \u003cth\u003e特点\u003c\/th\u003e\n      \u003c\/tr\u003e\n  \u003c\/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e冷板式\u003c\/td\u003e\n          \u003ctd\u003e将液冷冷板固定在服务器的主要发热器件上，依靠流经冷板的液体将热量带走达到散热目的。\u003c\/td\u003e\n          \u003ctd\u003e目前主流，占比超80%\u003cimg src=\u0022https:\/\/i-blog.csdnimg.cn\/blog_migrate\/eed5dc82a5d2a72611d9f51f68a31043.png\u0022 alt=\u0022在这里插入图片描述\u0022\u003e\u003c\/td\u003e\n      \u003c\/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e浸没式\u003c\/td\u003e\n          \u003ctd\u003e将发热元件直接浸没在冷却液中，通过冷却液循环带走服务器等设备运行产生的热量。分类：单相、两相、直抵芯片\u003c\/td\u003e\n          \u003ctd\u003e未来大趋势\u003cimg src=\u0022https:\/\/i-blog.csdnimg.cn\/blog_migrate\/541140c7db0226a0a203a7b83c9bc58b.png\u0022 alt=\u0022在这里插入图片描述\u0022\u003e\u003c\/td\u003e\n      \u003c\/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003c\/td\u003e\n          \u003ctd\u003e\u003c\/td\u003e\n          \u003ctd\u003e\u003c\/td\u003e\n      \u003c\/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e喷淋式\u003c\/td\u003e\n          \u003ctd\u003e在机箱顶部储液和开孔，根据发热体位置和发热量大小不同，让冷却液对发热体进行喷淋，达到设备冷却的目的。\u003c\/td\u003e\n          \u003ctd\u003e\u003c\/td\u003e\n      \u003c\/tr\u003e\n  \u003c\/tbody\u003e\n\u003c\/table\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e关键技术\u003c\/th\u003e\n          \u003cth\u003e作用\u003c\/th\u003e\n          \u003cth\u003e详情\u003c\/th\u003e\n      \u003c\/tr\u003e\n  \u003c\/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e冷却液\u003c\/td\u003e\n          \u003ctd\u003e可用冷媒包括水、矿物油、电子氟化液等进行物理降温\u003c\/td\u003e\n          \u003ctd\u003e冷却液是关键的原材料之一。以前，冷却液基本上被3M为代表的国外企业垄断，如今国内企业在冷却液方面也取得了国产突破。巨化集团推出高性能巨芯专用冷却液,攻克了数据中心高效液冷与安全智能热控防护等关键技术，并实现批量生产。\u003c\/td\u003e\n      \u003c\/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e连接器\u003c\/td\u003e\n          \u003ctd\u003e通常是指液冷循环系统中各器件之间的连接件，它能 实现各器件之间的快速连接和断开且无泄漏，提高效率，减少排液注液带来的不必要的工作量。主 要应用于流体连接，可在带压状态下自由插拔且无泄漏。\u003c\/td\u003e\n          \u003ctd\u003e液冷快速连接器的可靠性和绝对密封性至关重要，其事关整个液冷系统的安全性，使得其具备较高的技术门槛和较多的技术要点，包括防水密封、高耐用度及材质兼容性等\u003c\/td\u003e\n      \u003c\/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eCDU（Cooling Dispensing Unit，冷却液分配单元\u003c\/td\u003e\n          \u003ctd\u003e主要由机箱、水泵、板换、阀、膨胀罐以及 管路等等组成，通过板换进行热量交换，把冷却的液体送到热源处吸收热量，带着热量的液体再进 入板换进行换热处理，循环换热。\u003c\/td\u003e\n          \u003ctd\u003e\u003cimg src=\u0022https:\/\/i-blog.csdnimg.cn\/blog_migrate\/1c3d1ea38ff2d61bf00f564a7bbe7542.png\u0022 alt=\u0022在这里插入图片描述\u0022\u003e\u003c\/td\u003e\n      \u003c\/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e电磁阀\u003c\/td\u003e\n          \u003ctd\u003e一般安装在连接的管道上控制冷媒流通。\u003c\/td\u003e\n          \u003ctd\u003e\u003c\/td\u003e\n      \u003c\/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eTANK\u003c\/td\u003e\n          \u003ctd\u003e用于安装服务器\/交换机的浸没式箱体，通过 tank 内的冷却介质直接对交换机进行散热\u003c\/td\u003e\n          \u003ctd\u003e\u003c\/td\u003e\n      \u003c\/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eManifold\u003c\/td\u003e\n          \u003ctd\u003e分集水器，用于连接各路加热管供回水的配、集水装臵，按进、回水方式不同分为分 水器和集水器。\u003c\/td\u003e\n          \u003ctd\u003e\u003c\/td\u003e\n      \u003c\/tr\u003e\n  \u003c\/tbody\u003e\n\u003c\/table\u003e\n\u003cp\u003e国内已经形成了较为完整的液冷数据中心产业链，不过由于上游零部件的工艺性能、技术水平等方面与进口产品还是存在差距，液冷数据中心产业链有待完善。液冷技术非常复杂，涉及液冷数据中心系统架构层、液冷部件及接口层、液冷基础设施层（液冷机柜、组件、换热设备、室外集成冷源等）、液冷监控系统层等多方面，产业链上各个企业技术路径多种多样、产品规格千差万别，产品质量良莠不齐，让用户难以选择。\u003c\/p\u003e


      


    ",
        "license": "",
        
        "inLanguage": "en",
        "isFamilyFriendly": "true",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/ostring.github.io\/posts\/data_center\/server_cooling_evolution\/server_cooling_evolution\/"
        },
        "author" : {
            "@type": "Person",
            "name": "OString"
        },
        "creator" : {
            "@type": "Person",
            "name": "OString"
        },
        "accountablePerson" : {
            "@type": "Person",
            "name": "OString"
        },
        "copyrightHolder" : {
            "@type": "Person",
            "name": "OString"
        },
        "dateCreated": "2025-05-22T22:12:04.31Z",
        "datePublished": "2025-05-22T22:12:04.00Z",
        "dateModified": "2025-05-22T22:12:04.00Z",
        "publisher":{
            "@type":"Organization",
            "name": "OString",
            "url": "https://ostring.github.io/",
            "logo": {
                "@type": "ImageObject",
                "url": "https:\/\/ostring.github.io\/favicon-32x32.png",
                "width":"32",
                "height":"32"
            }
        },
        "image": 
      [
      ]

    ,
        "url" : "https:\/\/ostring.github.io\/posts\/data_center\/server_cooling_evolution\/server_cooling_evolution\/",
        "wordCount" : "108",
        "genre" : [ ],
        "keywords" : [ ]
    }
  </script>


</head>
<body class="body">
    <div class="wrapper">
      <aside
        
          class="wrapper__sidebar"
        
      ><div
  class="sidebar
    animated fadeInDown
  "
>
  <div class="sidebar__content">
    <div class="sidebar__introduction">
      <img
        class="sidebar__introduction-profileimage"
        src="/images/profile.jpg"
        alt="profile picture"
      />
      
        <div class="sidebar__introduction-title">
          <a href="/"></a>
        </div>
      
      <div class="sidebar__introduction-description">
        <p>Welcome to OString's Website</p>
      </div>
    </div>
    <ul class="sidebar__list">
      
    </ul>
  </div><footer class="footer footer__sidebar">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        OString
        2025
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js"
    integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ="
    crossorigin="anonymous"
  ></script></div>
</aside>
      <main
        
          class="wrapper__main"
        
      >
        <header class="header"><div
  class="
    animated fadeInDown
  "
>
  <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
  </a>
  <nav class="nav">
    <ul class="nav__list" id="navMenu">
      
      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/"
              
              title=""
              >Home</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/posts/data_center/"
              
              title=""
              >DataCenter</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/posts/tech_news/"
              
              title=""
              >TechNews</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/about/"
              
              title=""
              >About</a
            >
          </li>
        

      
    </ul>
    <ul class="nav__list nav__list--end">
      
      
        <li class="nav__list-item">
          <div class="themeswitch">
            <a title="Switch Theme">
              <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a>
          </div>
        </li>
      
    </ul>
  </nav>
</div>
</header>
  <div
    class="post 
      animated fadeInDown
    "
  >
    
    <div class="post__content">
      
        <h1></h1>
      
      <h1 id="服务器散热进化史">服务器散热进化史</h1>
<h3 id="液冷方案崛起">液冷方案崛起</h3>
<p>最近在关注美股，围观行业大佬分析NV 供应链体系，注意到一家公司VRT， 这家公司是NV 的独家液冷方案提供商。最近VRT 的股价青云直上，一家做液冷方案的公司护城河到底有多深，散热方案在整个行业中处于什么位置，这些问题是我想了解的。</p>
<p>NV 的技术路线图从A100/H100 到最新GB200 的体系，TDP 也从300~700W 跃升到1000W 的门槛，以往的风冷方案在散热效率上显得非常力不从心。从2019 年的数据来看，国内IDC 能耗43%用于 IT 设备散热， 基本与 45%的 IT 设备自身能耗持平，这在低碳的大背景下，与IDC 行业追求的PUE 小于1.3 相背离。</p>
<blockquote>
<p>全国数据中心 PUE 平均水平为 1.49，传统风冷数据中心 PUE 在 1.5<del>1.8（1kW 的服务器正常运转需 1.5</del>1.8kW 电量）</p></blockquote>
<h3 id="传统风冷">传统风冷</h3>
<p>从目前所有可见的消息来看，提高目前IDC风冷方案散热效率都是大势所趋，液冷方案呼之欲出，想要部署密度极高机架（30kW以上）的设施在是否使用液冷方面几乎没有选择余地。无论如何配置或优化系统，风冷都无法提供维持IT系统可靠性所需的散热能力。在边缘计算和核心数据中心都是如此。我们先来看看液冷方案相比于风冷有哪些优势：</p>
<ul>
<li>液体导热能力是空气的 25 倍，同体积液体带走热量是同体积空气的近 3000 倍，液冷技术可实现40~55℃高温供液，无需压缩机冷水机组，采用室外冷却塔，可实现全年自然冷却</li>
<li>在耗电量方面，液冷系统约比风冷系统节省电量 30~50%</li>
<li>生态环境的优势：液冷噪音低，能够降低冷却风机转速或者采用无风机设计，从而具备极佳的降噪效果，提升机房运维环境舒适性，解决噪声污染问题</li>
<li>液冷系统可以更高效地回收和再利用余热，有助于提升能源的总体利用效率</li>
<li>液冷服务器可以排除海拔、地域和气 温的差距，保证运行效率和性能，具备规模化应用的优势。</li>
</ul>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/c6290c440c9c31486781c45011396283.png" alt="在这里插入图片描述">
相比于风冷有这么多优势，为什么没有在IDC 发展初期就大范围铺开，从IDC 的发展历史来看，初期，一台1U 的刀片服务器主要高密度计算还集中在逻辑控制任务，再到后来的互联网时代，思科的崛起，IDC 服务器主要处理的是路由转发任务，这两类任务的一个显著特点是还集中在逻辑判断和少量的计算，映射到芯片上的逻辑单元是ALU 和开关电路，从芯片的工作状态来说，这类计算相关的能量负载是非集中式的，同时能量开销也很低。因此负担这类计算所需要的散热需求也不是难么急切，大多状况下，IDC 中的服务器散热可能主要依赖几台挂式空调就好，部署难度极地，从IT 运维的收益来说，并没有动力去推动部署液冷。
<img src="https://i-blog.csdnimg.cn/blog_migrate/3ddd8bab804ff39e885f8b344dc0fa70.png" alt="在这里插入图片描述">
从图中可以看到，芯片计算单元（MAC/ALU）小号的能量相比于DRAM数据搬运来说，小号的能量差了4个数量级，当下的智算中心所部署的服务器特点集中体现在高带宽（HBM/GDDR）,通常数据搬运的开销是整个网络执行的瓶颈所在。</p>
<h3 id="液冷的护城河">液冷的护城河</h3>
<p>回到液冷的讨论，当下的液冷解决方案供应商的技术护城河体现在哪里，为什么vrt 可以给到这么高的估值，并成为NV的唯一指定供应商。</p>
<p>目前主流的几种液冷方案：</p>
<table>
  <thead>
      <tr>
          <th>液冷技术类型</th>
          <th>描述</th>
          <th>特点</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>冷板式</td>
          <td>将液冷冷板固定在服务器的主要发热器件上，依靠流经冷板的液体将热量带走达到散热目的。</td>
          <td>目前主流，占比超80%<img src="https://i-blog.csdnimg.cn/blog_migrate/eed5dc82a5d2a72611d9f51f68a31043.png" alt="在这里插入图片描述"></td>
      </tr>
      <tr>
          <td>浸没式</td>
          <td>将发热元件直接浸没在冷却液中，通过冷却液循环带走服务器等设备运行产生的热量。分类：单相、两相、直抵芯片</td>
          <td>未来大趋势<img src="https://i-blog.csdnimg.cn/blog_migrate/541140c7db0226a0a203a7b83c9bc58b.png" alt="在这里插入图片描述"></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td>喷淋式</td>
          <td>在机箱顶部储液和开孔，根据发热体位置和发热量大小不同，让冷却液对发热体进行喷淋，达到设备冷却的目的。</td>
          <td></td>
      </tr>
  </tbody>
</table>
<table>
  <thead>
      <tr>
          <th>关键技术</th>
          <th>作用</th>
          <th>详情</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>冷却液</td>
          <td>可用冷媒包括水、矿物油、电子氟化液等进行物理降温</td>
          <td>冷却液是关键的原材料之一。以前，冷却液基本上被3M为代表的国外企业垄断，如今国内企业在冷却液方面也取得了国产突破。巨化集团推出高性能巨芯专用冷却液,攻克了数据中心高效液冷与安全智能热控防护等关键技术，并实现批量生产。</td>
      </tr>
      <tr>
          <td>连接器</td>
          <td>通常是指液冷循环系统中各器件之间的连接件，它能 实现各器件之间的快速连接和断开且无泄漏，提高效率，减少排液注液带来的不必要的工作量。主 要应用于流体连接，可在带压状态下自由插拔且无泄漏。</td>
          <td>液冷快速连接器的可靠性和绝对密封性至关重要，其事关整个液冷系统的安全性，使得其具备较高的技术门槛和较多的技术要点，包括防水密封、高耐用度及材质兼容性等</td>
      </tr>
      <tr>
          <td>CDU（Cooling Dispensing Unit，冷却液分配单元</td>
          <td>主要由机箱、水泵、板换、阀、膨胀罐以及 管路等等组成，通过板换进行热量交换，把冷却的液体送到热源处吸收热量，带着热量的液体再进 入板换进行换热处理，循环换热。</td>
          <td><img src="https://i-blog.csdnimg.cn/blog_migrate/1c3d1ea38ff2d61bf00f564a7bbe7542.png" alt="在这里插入图片描述"></td>
      </tr>
      <tr>
          <td>电磁阀</td>
          <td>一般安装在连接的管道上控制冷媒流通。</td>
          <td></td>
      </tr>
      <tr>
          <td>TANK</td>
          <td>用于安装服务器/交换机的浸没式箱体，通过 tank 内的冷却介质直接对交换机进行散热</td>
          <td></td>
      </tr>
      <tr>
          <td>Manifold</td>
          <td>分集水器，用于连接各路加热管供回水的配、集水装臵，按进、回水方式不同分为分 水器和集水器。</td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>国内已经形成了较为完整的液冷数据中心产业链，不过由于上游零部件的工艺性能、技术水平等方面与进口产品还是存在差距，液冷数据中心产业链有待完善。液冷技术非常复杂，涉及液冷数据中心系统架构层、液冷部件及接口层、液冷基础设施层（液冷机柜、组件、换热设备、室外集成冷源等）、液冷监控系统层等多方面，产业链上各个企业技术路径多种多样、产品规格千差万别，产品质量良莠不齐，让用户难以选择。</p>
<p>从业内的分析来看，目前产品的技术难点还在于连接器和冷却液以及配到的温控CDU 设备，但从行业发展规律看，这些技术难点相比于芯片，难度还是在可控范围内，国内迟早都可以突破，目前国内的技术发展还是有赖于产业的牵引，IDC 从风冷到液冷的坚定迁移（毕竟涉及到一次性的初期投入），才能保证液冷产业链的自身持续造血。</p>
<h1 id="参考资料">参考资料</h1>
<ul>
<li>液冷服务器行业专题研究：低碳之下，IDC“液冷时代”契机来临:https://new.qq.com/rain/a/20220923A02UQ100</li>
<li>VRT 技术白皮书：https://www.vertiv.cn/498c95/globalassets/documents/white-papers/vertiv_data_center_liquid_cooling_solutions_white_paper_350067_0.pdf</li>
<li><a href="https://zhuanlan.zhihu.com/p/661617328">https://zhuanlan.zhihu.com/p/661617328</a></li>
<li><a href="https://xueqiu.com/3966435964/281902854">https://xueqiu.com/3966435964/281902854</a></li>
</ul>
</div>
    <div class="post__footer">
      

      
    </div>

    
  </div>

      </main>
    </div><footer class="footer footer__base">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        OString
        2025
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js"
    integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ="
    crossorigin="anonymous"
  ></script></body>
</html>
