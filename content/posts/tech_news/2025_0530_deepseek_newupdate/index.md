
# DeepSeek 对国产芯片的价值

DeepSeek 又更新了，0528的版本在代码生成和幻觉消除方面有了比较大的提升：

>更新后的 R1 模型在数学、编程与通用逻辑等多个基准测评中取得了当前国内所有模型中首屈一指的优异成绩，并且在整体表现上已接近其他国际顶尖模型，如 o3 与 Gemini-2.5-Pro。

![](2025-05-30-15-16-53.png)


# DeepSeek 带来了什么

## 模型架构上的确定性，带来了软硬件一体化设计的确定性

DeepSeek 从DeepSeek V3 开始，包括后来的R1初版以及最新的0528 版本，都还是复用一套base架构，这种模型架构上的确定性给国产芯片的软硬件一体化指明了优化方向。

要知道，在这之前，本着好料用在刀刃上原则，为了支持所谓的主流模型，大多数芯片厂商在初期照着`bert` 和 `resnet` 规划`conv2d`,`conv1d` 的卷积算力，以及匹配的带宽，数据流的设计也是偏重于卷积，算力分配上基本也都是以dense 算力为主。再到后来，`llama` 系列的开源，Transformer 结构模型慢慢收敛，算力分配上又慢慢向`mutmul` 等算子进行建模。

除了底层算力配比的层面，在软件架构的层面，以往`cv` 类模型和想在`llm` 模型相比, 在上层架构上没有太多玩法，目前切到`Transformer` 类模型后，其对算力和带宽在`prefill` 和`decode`阶段的不同偏重，又引申出`P-D 分离`等上层框架的适配工作，比如`vLLM/SG-lang`等。


也许你会说`llama` 系列也是开源模型，同时架构也



## 